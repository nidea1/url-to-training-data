# ============================================================================
# BDO Data Extraction Pipeline - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your credentials
# NEVER commit .env to version control - it contains sensitive data!
# ============================================================================

# ============================================================================
# REQUIRED: Google Generative AI (LLM Generation)
# ============================================================================
# API Key for LLM-based dialogue generation
# Get your API key from: https://makersuite.google.com/app/apikey
# 1. Go to https://makersuite.google.com/app/apikey
# 2. Click "Create API key" 
# 3. Copy the key and paste below
GOOGLE_API_KEY=your_google_api_key_here

# ============================================================================
# REQUIRED: Hugging Face API Token (for web scraping)
# ============================================================================
# Get your API token from: https://huggingface.co/settings/tokens
# 1. Go to https://huggingface.co/settings/tokens
# 2. Click "New token", set role to "Read"
# 3. Copy the token and paste below
HF_TOKEN=your_huggingface_token_here

# ============================================================================
# SCRAPING CONFIGURATION
# ============================================================================
# Maximum requests per minute (rate limiting to avoid IP blocks)
SCRAPER_RATE_LIMIT=20

# Request timeout in seconds
SCRAPER_TIMEOUT=180

# ============================================================================
# TEXT CHUNKING CONFIGURATION
# ============================================================================
# Maximum tokens per chunk (keep under model's context limit)
CHUNKING_MAX_TOKENS=3500

# Number of list items per chunk when splitting long lists
CHUNKING_LIST_ITEMS_PER_CHUNK=12

# Number of table rows per chunk when splitting long tables
CHUNKING_TABLE_ROWS_PER_CHUNK=8

# Number of nested bullet groups per chunk
CHUNKING_NESTED_GROUPS_PER_CHUNK=8

# Minimum list items to trigger list splitting (25+ items = long list)
CHUNKING_MIN_LONG_LIST_ITEMS=25

# Minimum table rows to trigger table splitting (15+ rows = long table)
CHUNKING_MIN_LONG_TABLE_ROWS=15

# Minimum nested groups to trigger nested bullet table detection
CHUNKING_MIN_NESTED_GROUPS=5

# Minimum nested items to trigger nested bullet table detection
CHUNKING_MIN_NESTED_ITEMS=12

# ============================================================================
# LLM GENERATION CONFIGURATION
# ============================================================================
# Model selection
# Options: "gemini-2.0-flash" (faster), "gemini-2.0-pro" (more capable),
#          "gemini-1.5-pro", "gemini-1.5-flash", "gemma-3-27b-it"
GENERATION_MODEL_NAME=gemma-3-27b-it

# Temperature: 0-2 (lower = more deterministic, higher = more creative)
GENERATION_TEMPERATURE=0.6

# Top-P (nucleus sampling): 0-1 (higher = more diverse)
GENERATION_TOP_P=0.85

# Top-K (diversity parameter): 1-100 (lower = more deterministic)
GENERATION_TOP_K=32

# Maximum output tokens (limit for cost/speed)
GENERATION_MAX_OUTPUT_TOKENS=10240

# Retry limit on generation failure
GENERATION_RETRY_LIMIT=1

# ============================================================================
# QUALITY RULES CONFIGURATION
# ============================================================================
# Minimum dialogue pairs per chunk
QUALITY_MIN_PAIRS_PER_CHUNK=10

# Maximum dialogue pairs per chunk
QUALITY_MAX_PAIRS_PER_CHUNK=30

# Minimum words in user questions
QUALITY_MIN_INSTRUCTION_WORDS=6

# Maximum words in user questions
QUALITY_MAX_INSTRUCTION_WORDS=48

# Minimum words in assistant answers
QUALITY_MIN_OUTPUT_WORDS=14

# Maximum words in assistant answers
QUALITY_MAX_OUTPUT_WORDS=200

# ============================================================================
# FILE PATHS CONFIGURATION
# ============================================================================
# Input markdown file with links (for batch mode)
PATHS_MARKDOWN_FILENAME=./official/LINKS.md

# File to track processed URLs
PATHS_PROCESSED_LINKS_FILE=./official/processed_links.txt

# Output JSONL file with generated training data
PATHS_OUTPUT_FILENAME=bdo_official_guides.jsonl

# Log file for short chunks (less than min threshold)
PATHS_SHORT_CHUNKS_LOG=./short_chunks.log

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
# HuggingFace tokenizer model name (for accurate token counting)
# Common options: "google/gemma-3-27b-it", "meta-llama/Llama-2-7b"
MODEL_TOKENIZER_NAME=google/gemma-3-27b-it

# Optional: Path to custom meta prompt file
# If not set, uses default prompt from config.py
# MODEL_META_PROMPT_FILE=/path/to/custom_prompt.txt

# ============================================================================
# APPLICATION BEHAVIOR CONFIGURATION
# ============================================================================
# Debug mode: true for verbose logging, false for normal operation
APP_DEBUG_MODE=false

# Batch mode: true to process multiple URLs from file, false for single URL
APP_BATCH_MODE=true

# Single URL to process (used when APP_BATCH_MODE=false)
APP_TARGET_URL=https://www.naeu.playblackdesert.com/en-US/Wiki?wikiNo=259
